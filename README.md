# Grab DAX Voice Assistant â€“ Handsfree AI Co-Pilot

Prototype presented by team Pikachu

## ğŸ§  Overview

This project is a **voice-controlled assistant prototype** designed for **Grab driver-partners (DAX)**, enabling **hands-free interactions** with the Grab platform. It empowers drivers with AI support in **noisy, real-world environments**, ensuring both **safety** and **productivity** on the road.

Built for the **UMHackathon 2025**, this solution addresses the *"Economic Empowerment through AI"* theme by allowing DAX users to:
- Navigate efficiently ğŸš—
- Accept ride requests ğŸ›ï¸
- Chat with passengers ğŸ’¬
- Mark passengers as fetched âœ…
- Control these features via **voice commands** ğŸ—£ï¸

## ğŸ¯ Problem Statement

Drivers currently rely on manual input or screen-based interfaces, which are unsafe while driving. This assistant solves that by:
- Supporting **voice-first interactions**
- Functioning in **challenging audio conditions**
- Adapting to **regional dialects, accents, and colloquialisms**
- Delivering **real-time transcription and intent detection**
- Providing **audio feedback** in local languages

## âœ¨ Features

- ğŸ”Š **Noise-Resilient Voice Recorder** with real-time VAD + noise suppression
- ğŸ§  **Intent Prediction Engine** using language-detect + LLM (Gemma via Ollama)
- ğŸ“£ **Multilingual Text-to-Speech (TTS)** with support for English, Malay, and Chinese
- ğŸ¨ Dual-theme GUI (Light & Dark modes)
- ğŸ“± Android-style GUI with stacked pages and custom buttons
- ğŸ”„ Context-aware navigation (back/home/intents)

## ğŸ§© Architecture
```
Voice Input â†’ [Noise Reduction + VAD] â†’ Whisper Transcription â†’ â†’ Language Detection â†’ LLM Intent Classification â†’ â†’ UI Navigation / Voice Feedback (Edge-TTS)
```


## ğŸ—ï¸ Modules

| Module | Description |
|--------|-------------|
| `audio_recorder.py` | Handles voice activity detection, noise reduction, and WAV recording |
| `transcription.py` | Uses OpenAI Whisper for speech-to-text |
| `intent_predictor.py` | LLM-based intent classification with multilingual prompt support |
| `tts_engine.py` | Uses Edge TTS for responsive speech synthesis |
| `main_app.py` | PyQt5 GUI with interactive pages and theme switching |

## ğŸ§ª Key Technologies

- `PyQt5` â€“ for GUI components
- `webrtcvad`, `noisereduce`, `sounddevice` â€“ for audio preprocessing
- `whisper` â€“ for transcription
- `langdetect`, `langchain`, `Ollama` â€“ for language & intent modeling
- `pygame`, `edge-tts` â€“ for multilingual TTS

## ğŸ§  AI Intelligence

- **Zero-Shot Intent Recognition** using `distilbert` (fallback)
- **Multilingual Prompt Templates** for language-specific intent grounding
- **Colloquial Slang & Accent Adaptability** (via LLM prompt tuning)

## ğŸŒ Environmental Resilience

Tested under:
- ğŸš¦ Urban noise & engine rumble
- ğŸŒ§ï¸ Rain & wind simulations
- ğŸ”Š High-traffic scenarios (80â€“90 dB)
- ğŸ‡²ğŸ‡¾ Regional accents & speech quirks

## ğŸ¥ Demo

A short demo video is included to illustrate:
- Voice interaction workflow
- Intent recognition
- Multilingual feedback
- UI transitions & safety logic

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9+
- Ollama

### Setup

download gemma3
```commandline
Ollama run gemma3:latest
```
Install Python Dependencies
```commandline
pip install -r srcs/requirements.txt
```

### Run the App

```bash
python srcs/main_app.py
```

## ğŸ›¡ï¸ Safety & Ethics
- Hands-free only: No visual distractions for drivers
- Polite fallback prompts to clarify misheard commands
- Avoids unsafe instructions by design

## ğŸ“‚ Directory Structure
```
srcs
â”œâ”€â”€ main_app.py               # GUI logic
â”œâ”€â”€ audio_recorder.py         # Audio capture and VAD
â”œâ”€â”€ transcription.py          # Whisper-based STT
â”œâ”€â”€ intent_predictor.py       # LLM-based intent detection
â”œâ”€â”€ tts_engine.py             # Edge-TTS for feedback
â””â”€â”€ data/                     # Recorded audio + TTS outputs
```

## ğŸ§  Future Improvements

- Next-Level Noise Reduction with Krisp API

- Switching to Locally trained Malaysian-Centric STT Model

- Adding AI Memory for Context awareness and personalization

## ğŸ“¸ Snapshots

### ğŸŸ¢ Normal Mode â€“ Home Dashboard
Voice mode is off, and the driver can interact with the interface using buttons.

![normal mode](snapshots%2Fimg.png)

### ğŸ¤ Voice Mode Activated
The app switches to dark theme when voice mode is active to signal listening state.

![voice mode](snapshots%2Fimg_1.png)

### ğŸ§ª Example Voice Interactions

ğŸ§  The assistant supports multilingual commands (English, Chinese, Malay) and can understand colloquial instructions, returning relevant actions with natural voice feedback.

```commandline
Transcribed [recorded_audio_1.wav]: Can you navigate me to the closest hospital?
[Edge-TTS] Speaking in en: Okay, navigating you to the closest hospital. Just one momentâ€¦
(Slight pause â€“ simulating map lookup)
Okay, the closest hospital is Pusat Perubatan Universiti Malaya, approximately 5 kilometers away. Iâ€™m sending the route to your navigation system now.
Predicted intent: navigation
```
```
Transcribed [recorded_audio_1.wav]: è·Ÿä¹˜å®¢è®²æ¨¡è¦åˆ°äº†å°±è¦åˆ°å¿«ç‚¹å‡ºæ¥ç­‰
[Edge-TTS] Speaking in zh-cn: å¥½çš„ï¼Œæ˜ç™½ã€‚
â€œå¥½çš„ï¼Œæˆ‘æ¥å¸®æ‚¨è·Ÿä¹˜å®¢æ²Ÿé€šã€‚æ‚¨è¯´â€˜æ¨¡è¦åˆ°äº†å°±è¦åˆ°å¿«ç‚¹å‡ºæ¥ç­‰â€™ï¼Œæˆ‘æ¥ä»£æ›¿æ‚¨è¯´ï¼šâ€˜ä¹˜å®¢ï¼Œè¯·æ‚¨å°½å¿«å‡ºæ¥ç­‰å¾…ã€‚â€™ ç¨åæˆ‘ä¼šæé†’æ‚¨æ³¨æ„å®‰å…¨ã€‚â€
Predicted intent: chat_passenger
```
```
Transcribed [recorded_audio_2.wav]: ç¿»å›ä¸»ç•Œé¢
[Edge-TTS] Speaking in cn: å¥½çš„ï¼Œæ²¡é—®é¢˜ã€‚
â€œå¥½çš„ï¼Œæ­£åœ¨è¿”å›ä¸»ç•Œé¢ã€‚â€
Predicted intent: back
```
