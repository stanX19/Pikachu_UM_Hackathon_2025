# Grab DAX Voice Assistant â€“ Handsfree AI Co-Pilot

UM Hackathon 2025 Prototype presented by team Pikachu

## ðŸ“š Table of Contents

- [ðŸ§  Overview](#-overview)
- [ðŸŽ¯ Problem Statement](#-problem-statement)
- [âœ¨ Features](#-features)
- [ðŸ§© Architecture](#-architecture)
- [ðŸ—ï¸ Modules](#-modules)
- [ðŸ§ª Key Technologies](#-key-technologies)
- [ðŸ§  AI Intelligence](#-ai-intelligence)
- [ðŸŽ¥ Demo](#-demo)
- [ðŸš€ Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Setup](#setup)
  - [Run the App](#run-the-app)
- [ðŸ›¡ï¸ Safety & Ethics](#-safety--ethics)
- [ðŸ“‚ Directory Structure](#-directory-structure)
- [ðŸ§  Future Improvements](#-future-improvements)
- [ðŸ“¸ Snapshots](#-snapshots)
- [ðŸ“š Citations & References](#-citations--references)


## ðŸ§  Overview

This project is a **voice-controlled assistant prototype** designed for **Grab driver-partners (DAX)**, enabling **hands-free interactions** with the Grab platform. It empowers drivers with AI support in **noisy, real-world environments**, ensuring both **safety** and **productivity** on the road.

Built for the **UMHackathon 2025**, this solution addresses the *"Economic Empowerment through AI"* theme by allowing DAX users to:
- Navigate efficiently ðŸš—
- Accept ride requests ðŸ›Žï¸
- Chat with passengers ðŸ’¬
- Mark passengers as fetched âœ…
- Control these features via **voice commands** ðŸ—£ï¸

## ðŸŽ¯ Problem Statement

Drivers currently rely on manual input or screen-based interfaces, which are unsafe while driving. This assistant solves that by:
- Supporting **voice-first interactions**
- Functioning in **challenging audio conditions**
- Adapting to **regional dialects, accents, and colloquialisms**
- Delivering **real-time transcription and intent detection**
- Providing **audio feedback** in local languages

## âœ¨ Features

- ðŸ”Š **Noise-Resilient Voice Recorder** with real-time VAD + noise suppression
- ðŸ§  **Intent Prediction Engine** using language-detect + LLM (Gemma via Ollama)
- ðŸ“£ **Multilingual Text-to-Speech (TTS)** with support for English, Malay, and Chinese
- ðŸŽ¨ Dual-theme GUI (Light & Dark modes)
- ðŸ“± Android-style GUI with stacked pages and custom buttons
- ðŸ”„ Context-aware navigation (back/home/intents)

## ðŸ§© Architecture
```
Voice Input â†’ [Noise Reduction + VAD] â†’ Whisper Transcription â†’â†’
Language Detection â†’ LLM Intent Classification â†’ â†’ UI Navigation / Voice Feedback (Edge-TTS)
```


## ðŸ—ï¸ Modules

| Module | Description |
|--------|-------------|
| `audio_recorder.py` | Handles voice activity detection, noise reduction, and WAV recording |
| `transcription.py` | Uses OpenAI Whisper for speech-to-text |
| `intent_predictor.py` | LLM-based intent classification with multilingual prompt support |
| `tts_engine.py` | Uses Edge TTS for responsive speech synthesis |
| `main_app.py` | PyQt5 GUI with interactive pages and theme switching |

## ðŸ§ª Key Technologies

- `PyQt5` â€“ for GUI components
- `webrtcvad`, `noisereduce`, `sounddevice` â€“ for audio preprocessing
- `whisper` â€“ for transcription
- `langdetect`, `langchain`, `Ollama` â€“ for language & intent modeling
- `pygame`, `edge-tts` â€“ for multilingual TTS

## ðŸ§  AI Intelligence

- **Zero-Shot Intent Recognition** using `distilbert` (fallback)
- **Multilingual Prompt Templates** for language-specific intent grounding
- **Colloquial Slang & Accent Adaptability** (via LLM prompt tuning)

## ðŸŽ¥ Demo

A short demo video is included to illustrate:
- Voice interaction workflow
- Intent recognition
- Multilingual feedback
- UI transitions & safety logic

[link to video]

## ðŸš€ Getting Started

### Prerequisites

- Python 3.9+
- Ollama

### Setup

download gemma3
```commandline
Ollama run gemma3:latest
```
Install Python Dependencies
```commandline
pip install -r srcs/requirements.txt
```

### Run the App

```bash
python srcs/main_app.py
```

## ðŸ›¡ï¸ Safety & Ethics
- Hands-free only: No visual distractions for drivers
- Polite fallback prompts to clarify misheard commands
- Avoids unsafe instructions by design

## ðŸ“‚ Directory Structure
```
srcs
â”œâ”€â”€ main_app.py               # GUI logic
â”œâ”€â”€ audio_recorder.py         # Audio capture and VAD
â”œâ”€â”€ transcription.py          # Whisper-based STT
â”œâ”€â”€ intent_predictor.py       # LLM-based intent detection
â”œâ”€â”€ tts_engine.py             # Edge-TTS for feedback
â””â”€â”€ data/                     # Recorded audio + TTS outputs
```

## ðŸ§  Future Improvements

- Next-Level Noise Reduction with Krisp API

- Switching to Locally trained Malaysian-Centric STT Model

- Adding AI Memory for Context awareness and personalization

## ðŸ“¸ Snapshots

### ðŸŸ¢ Normal Mode â€“ Home Dashboard
Voice mode is off, and the driver can interact with the interface using buttons.

![normal mode](snapshots%2Fimg.png)

### ðŸŽ¤ Voice Mode Activated
The app switches to dark theme when voice mode is active to signal listening state.

![voice mode](snapshots%2Fimg_1.png)

### ðŸ§ª Example Voice Interactions

ðŸ§  The assistant supports multilingual commands (English, Chinese, Malay) and can understand colloquial instructions, returning relevant actions with natural voice feedback.

```commandline
Transcribed [recorded_audio_1.wav]: Can you navigate me to the closest hospital?
[Edge-TTS] Speaking in en: Okay, navigating you to the closest hospital. Just one momentâ€¦
(Slight pause â€“ simulating map lookup)
Okay, the closest hospital is Pusat Perubatan Universiti Malaya, approximately 5 kilometers away. Iâ€™m sending the route to your navigation system now.
Predicted intent: navigation
```
```
Transcribed [recorded_audio_1.wav]: è·Ÿä¹˜å®¢è®²æ¨¡è¦åˆ°äº†å°±è¦åˆ°å¿«ç‚¹å‡ºæ¥ç­‰
[Edge-TTS] Speaking in zh-cn: å¥½çš„ï¼Œæ˜Žç™½ã€‚
â€œå¥½çš„ï¼Œæˆ‘æ¥å¸®æ‚¨è·Ÿä¹˜å®¢æ²Ÿé€šã€‚æ‚¨è¯´â€˜æ¨¡è¦åˆ°äº†å°±è¦åˆ°å¿«ç‚¹å‡ºæ¥ç­‰â€™ï¼Œæˆ‘æ¥ä»£æ›¿æ‚¨è¯´ï¼šâ€˜ä¹˜å®¢ï¼Œè¯·æ‚¨å°½å¿«å‡ºæ¥ç­‰å¾…ã€‚â€™ ç¨åŽæˆ‘ä¼šæé†’æ‚¨æ³¨æ„å®‰å…¨ã€‚â€
Predicted intent: chat_passenger
```
```
Transcribed [recorded_audio_2.wav]: ç¿»å›žä¸»ç•Œé¢
[Edge-TTS] Speaking in cn: å¥½çš„ï¼Œæ²¡é—®é¢˜ã€‚
â€œå¥½çš„ï¼Œæ­£åœ¨è¿”å›žä¸»ç•Œé¢ã€‚â€
Predicted intent: back
```

## ðŸ“š Citations & References

This project builds upon a wide array of open-source tools, models, and libraries. We gratefully acknowledge the following:

---

- **Gemma** â€“ Google's lightweight LLM (via Ollama)
  > Google. *Gemma: Lightweight Open Models for Responsible AI*. 2024.  
  [arXiv:2403.10600](https://arxiv.org/abs/2403.10600)

- **Whisper** â€“ Multilingual Speech Recognition by OpenAI  
  > Radford et al. *Robust Speech Recognition via Large-Scale Weak Supervision*. 2022.  
  [arXiv:2212.04356](https://arxiv.org/abs/2212.04356)

- **DistilBERT** â€“ Transformer for fallback zero-shot classification  
  > Sanh, V., Debut, L., Chaumond, J., Wolf, T. *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*. 2019.  
  [arXiv:1910.01108](https://arxiv.org/abs/1910.01108)

- **LangChain** â€“ LLM orchestration framework  
  > Harrison Chase et al. *LangChain: Building Applications with LLMs through Composability*. 2023.  
  [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)

- **LangDetect**  
  > Nakatani, Shuyo. *Language Detection Library for Java (ported to Python)*. 2010.  
  [https://github.com/Mimino666/langdetect](https://github.com/Mimino666/langdetect)

- **PyQt5** â€“ Qt GUI framework for Python  
  > Riverbank Computing. *PyQt Documentation*.  
  [https://www.riverbankcomputing.com/software/pyqt/intro](https://www.riverbankcomputing.com/software/pyqt/intro)

- **WebRTC VAD**  
  > Google WebRTC. *Voice Activity Detection (VAD)*.  
  [https://webrtc.org](https://webrtc.org)

- **noisereduce**  
  > Tim Sainburg. *Noise reduction using spectral gating*. 2020.  
  [GitHub](https://github.com/timsainb/noisereduce)

- **SoundDevice**  
  > Matthias Geier. *python-sounddevice: PortAudio bindings for Python*.  
  [https://python-sounddevice.readthedocs.io](https://python-sounddevice.readthedocs.io)

- **Edge-TTS**  
  > Uses Microsoft Edge Neural Voices via unofficial API  
  [GitHub](https://github.com/rany2/edge-tts)

- **Pygame**  
  > Pygame Community. *Pygame â€“ Python Game Development*.  
  [https://www.pygame.org](https://www.pygame.org)

- **SciPy & NumPy**  
  > Virtanen, P. et al. *SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python*. 2020.  
  [Nature Methods, 17, 261â€“272](https://www.nature.com/articles/s41592-019-0686-2)

---